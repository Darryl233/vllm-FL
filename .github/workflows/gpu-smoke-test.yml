name: GPU Silicon Smoke Test

on:
  push:
    branches:
      - main
  pull_request:
    branches: main
  workflow_dispatch:  # Manual trigger

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}-${{ github.actor }}
  cancel-in-progress: true

jobs:
  ubuntu-gpu-smoke-test:
    runs-on: [ self-hosted, Linux, X64, gpu-8, nvidia ]
    timeout-minutes: 60
    container:
      image: harbor.baai.ac.cn/flagscale/cuda12.8.1-torch2.8.0-python3.10-vllm0.11.0_fl.0.1:20260205
      volumes:
        - /home/flagscale_cicd/docker/docker_build/docker_tokenizers:/home/gitlab-runner/tokenizers
        - /home/flagscale_cicd/docker/docker_build/docker_data:/home/gitlab-runner/data
      options: --gpus all --shm-size=500g --hostname flagscale_cicd --user root --ulimit nofile=65535:65535

    steps:
      - name: Checkout Code
        uses: actions/checkout@v6.0.1
        with:
          repository: ${{ github.event.pull_request.head.repo.full_name }}
          ref: ${{ github.event.pull_request.head.ref }}
          ssh-strict: true
          ssh-user: git
          persist-credentials: true
          clean: true
          sparse-checkout-cone-mode: true
          fetch-tags: false
          show-progress: true
          lfs: false
          submodules: false
          set-safe-directory: true

      - name: Install dependencies and build vLLM
        shell: bash
        run: |
          source /opt/miniconda3/etc/profile.d/conda.sh
          conda activate flagscale-inference
          pip install -r requirements/cuda.txt
          pip install -r requirements/common.txt
          pip install -e . -vvv
        env:
          CMAKE_BUILD_PARALLEL_LEVEL: 4

      - name: Verify installation
        shell: bash
        run: |
          source /opt/miniconda3/etc/profile.d/conda.sh
          conda activate flagscale-inference
          python -c "import vllm; print(f'vLLM version: {vllm.__version__}')"

      - name: Smoke test vllm serve
        shell: bash
        run: |
          source .github/scripts/gpu_check.sh
          wait_for_gpu

          source /opt/miniconda3/etc/profile.d/conda.sh
          conda activate flagscale-inference

          # Start server in background
          vllm serve /home/gitlab-runner/data/Qwen2.5-7B-Instruct \
            --gpu-memory-utilization=0.9 \
            --max-model-len=32768 \
            --max-num-seqs=256 \
            --port=8000 \
            --trust-remote-code \
            --enable-chunked-prefill &

          SERVER_PID=$!

          # Wait for server to start
          for i in {1..30}; do
            if curl -s http://localhost:8000/health > /dev/null; then
              echo "Server started successfully"
              break
            fi
            if [ "$i" -eq 30 ]; then
              echo "Server failed to start"
              kill "$SERVER_PID"
              exit 1
            fi
            sleep 3
          done

          # Test health endpoint
          curl -f http://localhost:8000/health

          # Test completion
          curl -f http://localhost:8000/v1/completions \
            -H "Content-Type: application/json" \
            -d '{
              "model": "/home/gitlab-runner/data/Qwen2.5-7B-Instruct",
              "prompt": "Hello",
              "max_tokens": 5
            }'

          # Cleanup
          kill "$SERVER_PID"
